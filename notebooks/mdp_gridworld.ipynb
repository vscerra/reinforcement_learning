{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eed45a1-bd18-4709-8319-18d0d9a76b6f",
   "metadata": {},
   "source": [
    "# Exploring Markov Decision Processes (MDPs) Through a Gridworld Example\n",
    "\n",
    "This notebook takes us through a gridworld explorer that allows us to see how states and actions affect transitions. This is also useful for observing reward dynamics and terminations, and helps us understand stochastic transitions in action. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ec111-5acf-44f8-9083-1a11207c6e26",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae31f15-a894-4303-9dae-539d51f486df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "from utils.gridworld import Gridworld\n",
    "from utils.gridworld_solver import value_iteration\n",
    "from utils.plot_utils import plot_value_heatmap, plot_policy_arrows\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "env = Gridworld(size=4, start=0, goal=15, traps=[5, 11])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff34563-2ffa-4615-96a1-4107fa873f5f",
   "metadata": {},
   "source": [
    "## Visualize the Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0d81c6-fbdb-42d3-ac04-585ffeb00d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_grid(state):\n",
    "    grid = np.zeros((4, 4), dtype=str)\n",
    "    grid[:, :] = \".\"\n",
    "    grid[env.goal // 4, env.goal % 4] = \"G\"\n",
    "    for trap in env.traps:\n",
    "        grid[trap // 4, trap % 4] = \"X\"\n",
    "    grid[state // 4, state % 4] = \"A\"\n",
    "\n",
    "    print(\"\\nGrid:\")\n",
    "    for row in grid:\n",
    "        print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60052f3b-a9ca-4bbf-8b99-d70e9c9d4f88",
   "metadata": {},
   "source": [
    "## Run Episode Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ad1d0-4ac3-44a7-87ba-a2bbee198c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "draw_grid(state)\n",
    "\n",
    "for t in range(20):\n",
    "    action = int(input(\"Action (0=up, 1=right, 2=down, 3=left): \"))\n",
    "    next_state, reward, done = env.step(action)\n",
    "    draw_grid(next_state)\n",
    "    print(f\"Step {t+1} - Reward: {reward}\")\n",
    "    if done:\n",
    "        print(\"Episode finished.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a65076-2815-40d2-9054-e85508b94f0f",
   "metadata": {},
   "source": [
    "## Solve Using Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04dba7-e8a1-4b08-8fa5-c06ff3a9d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "V, policy = value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dace8d-7f3c-40f4-b0ba-9680e604a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results\n",
    "plot_value_heatmap(V, env, title=\"Optimal State Values\")\n",
    "plot_policy_arrows(policy, env, title=\"Optimal Policy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
